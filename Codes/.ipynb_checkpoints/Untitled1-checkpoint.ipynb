{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-22138d4a3965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "glove_vectors_file = \"./glove/glove.6B.50d.txt\"\n",
    "glove_wordmap = {}\n",
    "\n",
    "with open(glove_vectors_file, \"r\",errors='ignore') as glove:\n",
    "    for line in glove:\n",
    "        name, vector = tuple(line.split(\" \", 1))\n",
    "        glove_wordmap[name] = np.fromstring(vector, sep=\" \")\n",
    "\n",
    "\n",
    "def sentence2sequence(sentence):\n",
    "    tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    rows1=[]\n",
    "    words = []\n",
    "    #Greedy search for tokens\n",
    "    for token in tokens:\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            if word in glove_wordmap:\n",
    "                rows.append(glove_wordmap[word])\n",
    "                words.append(word)\n",
    "                token =\"\"\n",
    "            else:\n",
    "                i = i-1\n",
    "    rows1=[item for sublist in rows for item in sublist]\n",
    "    rows1+=[0]*(750-len(rows1))\n",
    "    return rows1, words,len(rows1)\n",
    "\n",
    "def score_setup(row):\n",
    "    convert_dict = {\n",
    "      'entailment': 0,\n",
    "      'neutral': 1,\n",
    "      'contradiction': 2\n",
    "    }\n",
    "    score = np.zeros((3,))\n",
    "    for x in row['annotator_labels']:\n",
    "        if x in convert_dict: score[convert_dict[x]] += 1\n",
    "    return score / (1.0*np.sum(score))\n",
    "\n",
    "def split_data_into_scores(file_f):\n",
    "    with open(file_f,\"r\") as data:\n",
    "        jsonl_content = data.read()\n",
    "        data_decoded = [json.loads(jline) for jline in jsonl_content.split('\\n') if jline]\n",
    "        evi_sentences = []\n",
    "        hyp_sentences = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for row in data_decoded:\n",
    "            hyp_sentences.append(sentence2sequence(row['sentence2'].lower())[0])\n",
    "            evi_sentences.append(sentence2sequence(row['sentence1'].lower())[0])\n",
    "            scores.append(score_setup(row))\n",
    "        hyp_sentences = np.stack(hyp_sentences)\n",
    "        evi_sentences = np.stack(evi_sentences)\n",
    "        scores = np.stack(scores)\n",
    "        return (hyp_sentences, evi_sentences), scores\n",
    "\n",
    "data_feature_list,correct_scores=split_data_into_scores(\"./inputData/train.jsonl\")\n",
    "\n",
    "test_data_feature_list, test_correct_scores = split_data_into_scores(\"./inputData/test.jsonl\")\n",
    "\n",
    "dev_data_feature_list,dev_correct_scores=split_data_into_scores(\"./inputData/dev.jsonl\")\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_f = 500# 1st layer number of neurons\n",
    "n_output_f = 300# 2nd layer number of neurons\n",
    "n_hidden_g=50\n",
    "n_input = 15*50 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 3 # MNIST total classes (0-9 digits)\n",
    "\n",
    "#tf graph input\n",
    "X1 = tf.placeholder(\"float\", [None, n_input])\n",
    "X2 = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "#store layers weights and biases\n",
    "weights = {\n",
    "    'h_f': tf.Variable(tf.random_normal([n_hidden_f, n_input])),\n",
    "    'o_f': tf.Variable(tf.random_normal([n_output_f, n_hidden_f])),\n",
    "    'h_g': tf.Variable(tf.random_normal([n_hidden_g,n_output_f*2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes,n_hidden_g]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b_h_f': tf.Variable(tf.random_normal([n_hidden_f])),\n",
    "    'b_o_f': tf.Variable(tf.random_normal([n_output_f])),\n",
    "    'b_h_g': tf.Variable(tf.random_normal([n_hidden_g])),\n",
    "    'b_out': tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "}\n",
    "#create model\n",
    "def multilayer_perceptron(x1,x2):\n",
    "    layer_1_f_premise = tf.add(tf.matmul(x1,tf.transpose(weights['h_f'])), biases['b_h_f'])\n",
    "    layer_1_f_premise = tf.nn.relu(layer_1_f_premise)\n",
    "\n",
    "    layer_2_f_premise = tf.add(tf.matmul(layer_1_f_premise,tf.transpose(weights['o_f'])), biases['b_o_f'])\n",
    "    layer_2_f_premise = tf.nn.relu(layer_2_f_premise)\n",
    "\n",
    "    layer_1_f_hyp = tf.add(tf.matmul(x2, tf.transpose(weights['h_f'])), biases['b_h_f'])\n",
    "    layer_1_f_hyp = tf.nn.relu(layer_1_f_hyp)\n",
    "\n",
    "    layer_2_f_hyp = tf.add(tf.matmul(layer_1_f_hyp, tf.transpose(weights['o_f'])), biases['b_o_f'])\n",
    "    layer_2_f_hyp = tf.nn.relu(layer_2_f_hyp)\n",
    "\n",
    "    f_out = tf.concat([layer_2_f_premise,layer_2_f_hyp],1)\n",
    "\n",
    "    layer_1_g=tf.add(tf.matmul(f_out,tf.transpose(weights['h_g'])),biases['b_h_g'])\n",
    "    layer_1_g=tf.nn.relu(layer_1_g)\n",
    "    out_layer=tf.nn.softmax(tf.add(tf.matmul(layer_1_g,tf.transpose(weights['out'])),biases['b_out']))\n",
    "    return out_layer\n",
    "\n",
    "#construct model\n",
    "logits = multilayer_perceptron(X1,X2)\n",
    "loss_op = tf.reduce_mean(tf.losses.mean_squared_error(logits,Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    iterations = 0\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        iterations += 1\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(data_feature_list[0].shape[0]/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            data_x1 = data_feature_list[0][i*batch_size: (i+1)*batch_size]\n",
    "            data_x2 = data_feature_list[1][i*batch_size: (i+1)*batch_size]\n",
    "            data_y = correct_scores[i*batch_size: (i+1)*batch_size]\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([train_op, loss_op], feed_dict={X1: data_x1,\n",
    "                                                            X2: data_x2,\n",
    "                                                            Y: data_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "    # Test model\n",
    "    pred = logits  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({\n",
    "                                    X1: data_feature_list[0],\n",
    "                                    X2: data_feature_list[1],\n",
    "                                    Y: correct_scores}))\n",
    "\n",
    "    print(\"Accuracy:\", accuracy.eval({\n",
    "                                    X1: test_data_feature_list[0],\n",
    "                                    X2: test_data_feature_list[1],\n",
    "                                    Y: test_correct_scores}))\n",
    "\n",
    "    print(\"Accuracy:\", accuracy.eval({\n",
    "                                    X1: dev_data_feature_list[0],\n",
    "                                    X2: dev_data_feature_list[1],\n",
    "                                    Y: dev_correct_scores}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
